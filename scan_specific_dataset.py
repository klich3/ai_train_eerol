#!/usr/bin/env python3
"""
ğŸ¯ EscÃ¡ner EspecÃ­fico de Dataset
================================

Analiza una carpeta especÃ­fica para determinar su tipo y estructura.
Ãštil para datasets muy grandes o para anÃ¡lisis detallado.

Author: Anton Sychev
Created: 2025-07-05
"""

import os
import sys
import json
from pathlib import Path
from typing import Dict, List, Tuple
import argparse

# Agregar ruta de mÃ³dulos
sys.path.append(str(Path(__file__).parent))
from auto_organize_datasets import SmartDatasetOrganizer


class SpecificDatasetScanner:
    """ğŸ¯ EscÃ¡ner para un dataset especÃ­fico"""
    
    def __init__(self):
        self.organizer = SmartDatasetOrganizer()
        self.supported_image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif'}
    
    def analyze_dataset(self, dataset_path: str) -> Dict:
        """ğŸ” Analiza un dataset especÃ­fico en detalle"""
        path = Path(dataset_path)
        
        if not path.exists():
            return {"error": f"La ruta {dataset_path} no existe"}
        
        if not path.is_dir():
            return {"error": f"La ruta {dataset_path} no es una carpeta"}
        
        print(f"ğŸ” ANALIZANDO DATASET: {path.name}")
        print("="*50)
        
        # AnÃ¡lisis bÃ¡sico
        dataset_type, info = self.organizer.detect_dataset_type(path)
        
        # AnÃ¡lisis detallado adicional
        detailed_info = self._detailed_analysis(path)
        
        # Combinar informaciÃ³n
        result = {
            "path": str(path),
            "name": path.name,
            "type": dataset_type,
            "basic_info": info,
            "detailed_info": detailed_info,
            "recommendations": self._get_recommendations(dataset_type, info, detailed_info)
        }
        
        return result
    
    def _detailed_analysis(self, path: Path) -> Dict:
        """ğŸ“Š AnÃ¡lisis detallado del dataset"""
        print(f"\nğŸ“Š ANÃLISIS DETALLADO...")
        
        # Estructura de archivos
        all_files = list(path.rglob("*"))
        directories = [f for f in all_files if f.is_dir()]
        files = [f for f in all_files if f.is_file()]
        
        # Extensiones de archivos
        extensions = {}
        for file in files:
            ext = file.suffix.lower()
            extensions[ext] = extensions.get(ext, 0) + 1
        
        # DistribuciÃ³n por subdirectorios
        subdir_distribution = {}
        for directory in directories:
            if directory != path:  # Excluir el directorio raÃ­z
                relative_path = directory.relative_to(path)
                image_count = len([f for f in directory.rglob("*") 
                                 if f.suffix.lower() in self.supported_image_extensions])
                if image_count > 0:
                    subdir_distribution[str(relative_path)] = image_count
        
        # TamaÃ±os de archivos
        file_sizes = []
        total_size = 0
        for file in files:
            try:
                size = file.stat().st_size
                file_sizes.append(size)
                total_size += size
            except:
                continue
        
        avg_file_size = sum(file_sizes) / len(file_sizes) if file_sizes else 0
        max_file_size = max(file_sizes) if file_sizes else 0
        min_file_size = min(file_sizes) if file_sizes else 0
        
        # AnÃ¡lisis de imÃ¡genes
        image_analysis = self._analyze_images(path)
        
        detailed = {
            "total_directories": len(directories),
            "total_files": len(files),
            "extensions": extensions,
            "subdir_distribution": subdir_distribution,
            "size_analysis": {
                "total_size_mb": total_size / (1024 * 1024),
                "avg_file_size_kb": avg_file_size / 1024,
                "max_file_size_mb": max_file_size / (1024 * 1024),
                "min_file_size_bytes": min_file_size
            },
            "image_analysis": image_analysis
        }
        
        # Imprimir informaciÃ³n detallada
        print(f"   ğŸ“ Directorios: {detailed['total_directories']}")
        print(f"   ğŸ“„ Archivos: {detailed['total_files']}")
        print(f"   ğŸ’¾ TamaÃ±o total: {detailed['size_analysis']['total_size_mb']:.1f} MB")
        print(f"   ğŸ“Š Extensiones mÃ¡s comunes:")
        
        sorted_extensions = sorted(extensions.items(), key=lambda x: x[1], reverse=True)
        for ext, count in sorted_extensions[:5]:
            print(f"      {ext or '(sin ext)'}: {count} archivos")
        
        if subdir_distribution:
            print(f"   ğŸ“‚ DistribuciÃ³n por subcarpetas:")
            for subdir, count in list(subdir_distribution.items())[:5]:
                print(f"      {subdir}: {count} imÃ¡genes")
        
        return detailed
    
    def _analyze_images(self, path: Path) -> Dict:
        """ğŸ–¼ï¸ AnÃ¡lisis especÃ­fico de imÃ¡genes"""
        image_files = []
        for ext in self.supported_image_extensions:
            image_files.extend(path.rglob(f"*{ext}"))
        
        if not image_files:
            return {"error": "No se encontraron imÃ¡genes"}
        
        # Analizar una muestra de imÃ¡genes para obtener dimensiones
        sample_size = min(20, len(image_files))
        dimensions = []
        
        try:
            import cv2
            for img_file in image_files[:sample_size]:
                try:
                    img = cv2.imread(str(img_file))
                    if img is not None:
                        h, w = img.shape[:2]
                        dimensions.append((w, h))
                except:
                    continue
        except ImportError:
            # Si OpenCV no estÃ¡ disponible, usar PIL
            try:
                from PIL import Image
                for img_file in image_files[:sample_size]:
                    try:
                        with Image.open(img_file) as img:
                            dimensions.append(img.size)
                    except:
                        continue
            except ImportError:
                return {"error": "No se puede analizar imÃ¡genes (falta OpenCV o PIL)"}
        
        if dimensions:
            widths = [d[0] for d in dimensions]
            heights = [d[1] for d in dimensions]
            
            analysis = {
                "total_images": len(image_files),
                "sample_analyzed": len(dimensions),
                "avg_width": sum(widths) / len(widths),
                "avg_height": sum(heights) / len(heights),
                "max_width": max(widths),
                "max_height": max(heights),
                "min_width": min(widths),
                "min_height": min(heights),
                "common_resolutions": self._find_common_resolutions(dimensions)
            }
            
            print(f"   ğŸ–¼ï¸ AnÃ¡lisis de imÃ¡genes ({analysis['sample_analyzed']} muestras):")
            print(f"      ResoluciÃ³n promedio: {analysis['avg_width']:.0f}x{analysis['avg_height']:.0f}")
            print(f"      Rango: {analysis['min_width']}x{analysis['min_height']} - {analysis['max_width']}x{analysis['max_height']}")
            
            return analysis
        
        return {"error": "No se pudieron analizar las dimensiones"}
    
    def _find_common_resolutions(self, dimensions: List[Tuple[int, int]]) -> Dict:
        """ğŸ” Encuentra resoluciones comunes"""
        resolution_count = {}
        for w, h in dimensions:
            res = f"{w}x{h}"
            resolution_count[res] = resolution_count.get(res, 0) + 1
        
        # Ordenar por frecuencia
        sorted_resolutions = sorted(resolution_count.items(), key=lambda x: x[1], reverse=True)
        return dict(sorted_resolutions[:5])  # Top 5
    
    def _get_recommendations(self, dataset_type: str, basic_info: Dict, detailed_info: Dict) -> List[str]:
        """ğŸ’¡ Genera recomendaciones para el dataset"""
        recommendations = []
        
        if dataset_type == "YOLO":
            recommendations.append("âœ… Dataset YOLO listo para entrenar")
            recommendations.append("ğŸ’¡ Verificar que todas las clases estÃ©n en classes.txt")
            
        elif dataset_type == "COCO":
            recommendations.append("âœ… Dataset COCO detectado")
            recommendations.append("ğŸ’¡ Validar estructura JSON antes del entrenamiento")
            
        elif dataset_type == "PASCAL_VOC":
            recommendations.append("ğŸ”„ Convertir a YOLO para mejor compatibilidad")
            recommendations.append("ğŸ’¡ Usar el convertidor automÃ¡tico")
            
        elif dataset_type == "PURE_IMAGES":
            recommendations.append("ğŸ“ Dataset sin anotaciones")
            recommendations.append("ğŸ’¡ Considerar anotar manualmente o usar herramientas automÃ¡ticas")
            
        elif dataset_type == "CLASSIFICATION":
            recommendations.append("ğŸ“Š Dataset de clasificaciÃ³n por carpetas")
            recommendations.append("ğŸ’¡ Verificar balance entre clases")
            
        elif dataset_type == "UNET":
            recommendations.append("ğŸ­ Dataset de segmentaciÃ³n U-Net")
            recommendations.append("ğŸ’¡ Verificar correspondencia imagen-mÃ¡scara")
            
        # Recomendaciones basadas en tamaÃ±o
        if basic_info.get('images', 0) < 100:
            recommendations.append("âš ï¸ Dataset pequeÃ±o - considerar data augmentation")
        elif basic_info.get('images', 0) > 10000:
            recommendations.append("ğŸ“ˆ Dataset grande - considerar dividir en train/val/test")
        
        # Recomendaciones basadas en tamaÃ±o de archivo
        total_size = detailed_info.get('size_analysis', {}).get('total_size_mb', 0)
        if total_size > 1000:  # > 1GB
            recommendations.append("ğŸ’¾ Dataset pesado - considerar optimizaciÃ³n de imÃ¡genes")
        
        return recommendations
    
    def save_analysis_report(self, analysis: Dict, output_path: str = None):
        """ğŸ’¾ Guarda el reporte de anÃ¡lisis"""
        if output_path is None:
            output_path = f"dataset_analysis_{analysis['name']}.json"
        
        with open(output_path, 'w') as f:
            json.dump(analysis, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ Reporte guardado en: {output_path}")
    
    def print_summary(self, analysis: Dict):
        """ğŸ“‹ Imprime resumen del anÃ¡lisis"""
        if "error" in analysis:
            print(f"âŒ Error: {analysis['error']}")
            return
        
        print(f"\nğŸ“‹ RESUMEN DEL ANÃLISIS")
        print("="*30)
        print(f"ğŸ“ Dataset: {analysis['name']}")
        print(f"ğŸ” Tipo: {analysis['type']}")
        print(f"ğŸ–¼ï¸ ImÃ¡genes: {analysis['basic_info']['images']}")
        print(f"ğŸ’¾ TamaÃ±o: {analysis['basic_info']['size_mb']:.1f} MB")
        
        print(f"\nğŸ’¡ RECOMENDACIONES:")
        for i, rec in enumerate(analysis['recommendations'], 1):
            print(f"  {i}. {rec}")


def main():
    """ğŸš€ FunciÃ³n principal"""
    parser = argparse.ArgumentParser(description="ğŸ¯ EscÃ¡ner especÃ­fico de dataset")
    parser.add_argument("path", help="Ruta del dataset a analizar")
    parser.add_argument("--save", "-s", help="Guardar reporte en archivo JSON")
    parser.add_argument("--detailed", "-d", action="store_true", help="Mostrar anÃ¡lisis detallado")
    
    args = parser.parse_args()
    
    scanner = SpecificDatasetScanner()
    
    # Analizar dataset
    analysis = scanner.analyze_dataset(args.path)
    
    # Mostrar resumen
    scanner.print_summary(analysis)
    
    # Guardar reporte si se solicita
    if args.save:
        scanner.save_analysis_report(analysis, args.save)
    
    print(f"\nâœ… AnÃ¡lisis completado!")


if __name__ == "__main__":
    if len(sys.argv) == 1:
        # Modo interactivo si no hay argumentos
        print("ğŸ¯ ESCÃNER ESPECÃFICO DE DATASET")
        print("="*40)
        
        dataset_path = input("ğŸ“ Ingresa la ruta del dataset: ").strip()
        
        if not dataset_path:
            print("âŒ Ruta vacÃ­a")
            sys.exit(1)
        
        scanner = SpecificDatasetScanner()
        analysis = scanner.analyze_dataset(dataset_path)
        scanner.print_summary(analysis)
        
        # Preguntar si guardar reporte
        save_response = input("\nğŸ’¾ Â¿Guardar reporte en JSON? (s/N): ").strip().lower()
        if save_response in ['s', 'si', 'sÃ­', 'yes', 'y']:
            filename = f"analysis_{Path(dataset_path).name}.json"
            scanner.save_analysis_report(analysis, filename)
    else:
        main()
